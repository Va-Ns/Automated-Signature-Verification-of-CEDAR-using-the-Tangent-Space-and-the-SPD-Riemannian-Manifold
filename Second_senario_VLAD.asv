%% Cleanliness is half of nobility

clear;clc;close all;
s = rng("default");

% Define the base directory and the specific directory name
baseDirectory = 'C:\Users\Nik_Vas\Documents\GitHub\Second_Riemannian_Senario';
WorkspaceDirectory = 'Workspace'; 

% Create the full path to the specific directory
WorkspaceDirectoryPath = fullfile(baseDirectory, WorkspaceDirectory);

% Check if the specific directory exists, and if not, create it
if ~exist(WorkspaceDirectoryPath, 'dir') 
    mkdir Workspace 
end
%% In case you need man power

% delete(gcp('nocreate'))
% maxWorkers = maxNumCompThreads;
% disp("Maximum number of workers: " + maxWorkers);
% pool=parpool(maxWorkers/2);

%% Load the data
Covariance_matrices = load("CEDAR_covs10_v5_new_thlauto.mat");

% Right now the data, in the form of covariance matrices, can be though of as points on the 
% Riemannian manifold.

% Sort out the Genuine from the Forgeries and delete the initial variable to save memory
G_CovMa = Covariance_matrices.CEDAR.G;
F_CovMa = Covariance_matrices.CEDAR.F;

clear Covariance_matrices

%% Create the vectors on the Tangent Plane

G_Vecs = VecCell(G_CovMa);
F_Vecs = VecCell(F_CovMa);

%% Learning Stage

numIters = 5;
numFolds = 2;

numNeighbors = [1 5 1 20 30 66];

for Neighbor = 1 : length(numNeighbors)

    % Create the directory name
    DirectoryName = sprintf('For_Neighbor_%d', numNeighbors(Neighbor));

    % Create the full directory path
    DirectoryPath = fullfile(WorkspaceDirectoryPath, DirectoryName);

    % Create the directory if it doesn't exist
    if ~exist(DirectoryPath, 'dir')
        
        mkdir(DirectoryPath);
        
    end


    fprintf('Now on Neighbor: %d \n',numNeighbors(Neighbor))

    tic
    for Iter = 1 : numIters
        
        fprintf('   Now in Iteration: %d for Neighbor: %d  \n',Iter,numNeighbors(Neighbor))
    
        % Randomly sample the indices every time for every folder. In this way, we retrieve half of 
        % the dataset's writers in a randomly sampled manner, here 27 Writers
        randIndices = randsample(size(G_Vecs,1),floor(0.5*size(G_Vecs,1)));
    
        % Use the random indices to shuffle the cell array
        G_Learning_Data = G_Vecs(randIndices,:);
        F_Learning_Data = F_Vecs(randIndices,:);
    
        % Using reverse logic, we keep for the Testing Data the Writers that are not present in the
        % random indices created above by deleting the sampled ones. So here we would have the 
        % remaining 28 Writers
        G_Testing_Data = G_Vecs;
        G_Testing_Data (randIndices,:) = [];
    
        F_Testing_Data = F_Vecs;
        F_Testing_Data(randIndices,:) = [];
    
        for Fold = 1 : numFolds
    
            fprintf('      Now in Fold: %d \n',Fold)
            
            % Partition the Genuine data into Training and Validation 
            G_Metadata = Train_Val_Split(G_Learning_Data,"GetAllMetadata",true);
    
            % Partition the Forgery data into Training and Validation 
            F_Metadata  = Train_Val_Split(F_Learning_Data,"GetAllMetadata",true);
            
            %% Form the ω(+) class 
            
            % In order to form the ω(+) class, we need to perform Dichotomy Transform between the 
            % Genuine signatures per writer. That means that if a writer has 17 training signatures 
            % references, we need to perform Dichotomy Transform between every possible combination 
            % of those 17 references.
    
            [G_Training_set,G_Val_set] = OmegaPlusFormation(G_Learning_Data,G_Metadata);
            
    
            %% Form the ω(-) class
    
             % The ω(-) class can be formed by different types. For example, Random Forgeries of a
             % person can be formed by pairing a Genuine sample of the questioned Writer with the
             % Genuine sample of another Writer. Here we employ a pairing between a Genuine sample 
             % of a Writer with the simulated-or-skilled sample of the same Writer.
    
             [F_Training_set, F_Val_set] = OmegaMinusFormation(G_Learning_Data,F_Learning_Data, ...
                                                                                            F_Metadata);
    
             %% Codebook Formation for the Training set
    
            [~,G_Train_Codebook] = kmeans(gpuArray(G_Training_set),floor(size(G_Training_set,1)/ ...
                                             size(G_Training_set,2)),"MaxIter", 100,"Replicates",10);
    
            [~,F_Train_Codebook] = kmeans(gpuArray(F_Training_set),floor(size(F_Training_set,1)/ ...
                                             size(F_Training_set,2)),"MaxIter",100,"Replicates",10);
    
            fprintf('        Finished producing Codebooks for the Training Set of Iter %d of Fold %d \n', ...
                                                                                          Iter,Fold)
    
             %% Codebook Formation for the Validation set
    
            [~,G_Val_Codebook] = kmeans(gpuArray(G_Val_set),floor(size(G_Training_set,1)/ ...
                                             size(G_Training_set,2)),"MaxIter",100,"Replicates",10);
    
            [~,F_Val_Codebook] = kmeans(gpuArray(F_Val_set),floor(size(F_Training_set,1)/ ...
                                             size(F_Training_set,2)),"MaxIter",100,"Replicates",10);
    
            fprintf(['        Finished producing Codebooks for the Validation Set of Iter %d of Fold %d ' ...
                                                                                    '\n'],Iter,Fold)
    
            %% Vectors of Locally Aggregated Descriptors (VLAD) Encoding
    
            % Concentrate the sets into a struct
            Data(Neighbor).G_Training_set = G_Training_set;
            Data(Neighbor).F_Training_set = F_Training_set;
            Data(Neighbor).G_Val_set = G_Val_set;
            Data(Neighbor).F_Val_set = F_Val_set;
    
            % Concentrate the Dictionaries into a struct
    
            Codebooks(Neighbor).G_Train_Codebook = G_Train_Codebook;
            Codebooks(Neighbor).F_Train_Codebook = F_Train_Codebook;
            Codebooks(Neighbor).G_Val_Codebook = G_Val_Codebook;
            Codebooks(Neighbor).F_Val_Codebook = F_Val_Codebook;
    
            VLAD(Neighbor).VLAD_Encoding{Iter,Fold} = VLADNV(Codebooks(Neighbor),Data(Neighbor), ...
                                                             "numNeighbors",numNeighbors(Neighbor));
    
            fprintf('        Finished VLAD Encoding of Iter %d of Fold %d for Neighbor : %d \n', ...
                                                                  Iter, Fold,numNeighbors(Neighbor))

    
            %% Create the labels for the data
             
            % Create the labels for the Training set of both Genuine and Forgery
            G_Training_Label = ones([1 ...
                size(VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.G_Train_VLAD_Encoding_normed,1)])';
            F_Training_Label = -ones([1 ...
                size(VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.F_Train_VLAD_Encoding_normed,1)])';
    
            % Create the labels for the Validation set of both Genuine and Forgery
            G_Val_Label = ones([1 ...
                      size(VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.G_Val_VLAD_Encoding_normed,1)])';
            F_Val_Label = -ones([1 ...
                      size(VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.F_Val_VLAD_Encoding_normed,1)])';
             
            % Append them into a unified matrix of labels
            Training_Labels = [G_Training_Label;F_Training_Label];
            Validation_Labels  = [G_Val_Label;F_Val_Label];
    
            %% Scenario 2 - Two separate populations
    
            Training_Matrix = [VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.G_Train_VLAD_Encoding_normed;
                              VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.F_Train_VLAD_Encoding_normed];
    
            Validation_Matrix = [VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.G_Val_VLAD_Encoding_normed;
                                VLAD(Neighbor).VLAD_Encoding{Iter,Fold}.F_Val_VLAD_Encoding_normed];
            %% Perform Hyperparameter Optimization on the SVM
               
            Mdl = fitcsvm(gpuArray(Training_Matrix),Training_Labels, ...
                               'OptimizeHyperparameters','all', ...
                               'HyperparameterOptimizationOptions', ...
                               struct('Optimizer','gridsearch','NumGridDivisions',20, ...
                               'MaxObjectiveEvaluations',60,'ShowPlots',false));
    
            % Create a struct that stores the Hyperparameters
    
            BestNeighborHyperParam(Neighbor).BestHyperparams(Iter).BoxConstraint{Fold} = ...
                                                              Mdl.ModelParameters.BoxConstraint;
    
            BestNeighborHyperParam(Neighbor).BestHyperparams(Iter).KernelFunction{Fold} = ... 
                                                              Mdl.ModelParameters.KernelFunction;
    
            BestNeighborHyperParam(Neighbor).BestHyperparams(Iter).KernelScale{Fold} = ... 
                                                              Mdl.ModelParameters.KernelScale;
    
            BestNeighborHyperParam(Neighbor).BestHyperparams(Iter).PolynomialOrder{Fold} = ... 
                                                          Mdl.ModelParameters.KernelPolynomialOrder;
    
            BestNeighborHyperParam(Neighbor).BestHyperparams(Iter).KernelOffset{Fold} = ... 
                                                              Mdl.ModelParameters.KernelOffset;
    
            BestNeighborHyperParam(Neighbor).BestHyperparams(Iter).Standardize{Fold} = ... 
                                                              Mdl.ModelParameters.StandardizeData;
       
            %% Retrain the best found SVM
    
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).Best_Model{Fold} = ...
                                 fitcsvm(gpuArray(Training_Matrix),Training_Labels, ...
                                 'BoxConstraint'  ,     BestHyperparams(Iter).BoxConstraint{Fold}, ...
                                 'KernelFunction' ,     BestHyperparams(Iter).KernelFunction{Fold}, ...
                                 'KernelScale'    ,     BestHyperparams(Iter).KernelScale{Fold}, ...
                                 'PolynomialOrder',     BestHyperparams(Iter).PolynomialOrder{Fold}, ...
                                 'KernelOffset'   ,     BestHyperparams(Iter).KernelOffset{Fold}, ...
                                 'Standardize'    ,     BestHyperparams(Iter).Standardize{Fold});
    
             fprintf('          Finished training the SVM for Fold %d of Iter %d\n',Fold,Iter);
    
            %% Validation Stage
    
            [Predicted_Labels,Predicted_Scores] = ...
               predict(BestNeighborMdl(Neighbor).Best_Mdl(Iter).Best_Model{Fold},Validation_Matrix);
    
            [X,Y,T,AUC] = perfcurve(Validation_Labels,Predicted_Scores(:,2),1);
             
            FAR = X;
            FRR = 1 - Y;
    
            % Find the point where the difference between FPR and FNR is minimum (i.e., FPR ~ FNR)
            [~,minIndex] = min(abs(FAR - FRR));
    
            EER = FAR(minIndex); % or EER = FNR(minIndex);
             
            %% Store the data 
             
            % Create the testing indices by creating a vector with values from 1 to 55 and 
            % subtracting the indices that were used for the Learning set. 
            Testing_Indices = 1:size(G_CovMa,1); 
            Testing_Indices(randIndices) = [];
    
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).X{Fold} = X;
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).Y{Fold} = Y;
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).Scores{Fold} = Predicted_Scores;
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).AUC(Fold) = AUC;
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).EER(Fold) = EER;
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).Learning_Indices = randIndices;
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).Testing_Indices = Testing_Indices';
            BestNeighborMdl(Neighbor).Best_Mdl(Iter).Codebooks{Fold} = Codebooks;
         
            %% Change between the Testing and Learning Data for the K-Fold Cross Validation scheme
    
            % Here we transpose the Learning Data with the Testing Data, because in a 2-Fold cross
            % validation setup, the two sets are just flipped between them.
    
            tmp = G_Learning_Data;
            G_Learning_Data = G_Testing_Data; % <=  So here we expect G_Learning_Data to have 28
            %     Writers now
            G_Testing_Data = tmp; % <= Respectively, we expect G_Testing_Data to have 27 Writers now
    
            tmp = F_Learning_Data;
            F_Learning_Data = F_Testing_Data;
            F_Testing_Data = tmp;
            
        end
    
    end
    
    Learning_Stage_time(Neighbor) = toc 

    %% Save the data per Neighbor
    
    Directory = [DirectoryPath '\'];

        %% For the VLAD data

    fprintf('   Saving VLAD for Neighbor: %d', numNeighbors(Neighbor))
    FilenameVLAD = 'VLAD.mat';

    % Create the full file path
    fullFilePathVLAD = fullfile(Directory, FilenameVLAD);

    save(fullFilePathVLAD,"VLAD")

        %% For the Hyperparmateres
    
    fprintf('   Saving the Hyperparameters for Neighbor: %d', numNeighbors(Neighbor)')
    FilenameBestNeighbourHyperparam = 'BestNeighbourHyperparam.mat';

    % Create the full file path
    fullFilePathBestNeighbourHyperparam = fullfile(Directory, FilenameBestNeighbourHyperparam);

    save(fullFilePathBestNeighbourHyperparam,"BestNeighborHyperParam")

        %% For the Models
    
    fprintf('   Saving the Models for Neighbor: %d', numNeighbors(Neighbor)')
    FilenameBestNeighbourMdl = 'BestNeighbourMdl.mat';

    % Create the full file path
    fullFilePathBestNeighbourMdl = fullfile(Directory, FilenameBestNeighbourMdl);

    save(fullFilePathBestNeighbourMdl,"BestNeighborMdl")
   

end

%% Save the Collective Data

    %% For the Collective VLAD
fprintf('Saving Collective VLAD')
FilenameCollectiveVLAD = 'VLAD.mat';

% Create the full file path
fullFilePathCollectiveVLAD = fullfile(WorkspaceDirectory, FilenameCollectiveVLAD );

save(fullFilePathCollectiveVLAD,"VLAD")

    %% For the Collective Hyperparmateres

fprintf('Saving Collective Hyperparameters')
FilenameCollectiveBestNeighbourHyperparam = 'BestNeighbourHyperparam.mat';

% Create the full file path
fullFilePathCollectiveBestNeighbourHyperparam = fullfile(WorkspaceDirectory, FilenameCollectiveBestNeighbourHyperparam);

save(fullFilePathCollectiveBestNeighbourHyperparam,"BestNeighborHyperParam")

    %% For the Collective Models

fprintf('Saving Collective Models')
FilenameCollectiveBestNeighbourMdl = 'BestNeighbourMdl.mat';

% Create the full file path
fullFilePathCollectiveBestNeighbourMdl = fullfile(WorkspaceDirectory, FilenameCollectiveBestNeighbourMdl);

save(fullFilePathCollectiveBestNeighbourMdl ,"BestNeighborMdl")